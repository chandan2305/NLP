{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a string: chandan is a big boy\n",
      "[('chandan', 'NN'), ('big', 'JJ'), ('boy', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "txt = input(\"enter a string: \")\n",
    "\n",
    "tokenized = sent_tokenize(txt) \n",
    "for i in tokenized: \n",
    "\n",
    "    wordsList = nltk.word_tokenize(i) \n",
    " \n",
    "    wordsList = [w for w in wordsList if not w in stop_words] \n",
    "\n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "\n",
    "print(tagged) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.data import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Harry is swimming a big pool.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry', 'is', 'swimming', 'a', 'big', 'pool', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Harry', 'NNP')]\n",
      "[('is', 'VBZ')]\n",
      "[('swimming', 'VBG')]\n",
      "[('a', 'DT')]\n",
      "[('big', 'JJ')]\n",
      "[('pool', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time flies like an arrow. \n",
      "Fruit flies like a banana. \n",
      "Sam sat on the cat.\n",
      "The cat is white.\n"
     ]
    }
   ],
   "source": [
    "data = '''Time flies like an arrow. \n",
    "Fruit flies like a banana. \n",
    "Sam sat on the cat.\n",
    "The cat is white.'''\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time flies like an arrow. ',\n",
       " 'Fruit flies like a banana. ',\n",
       " 'Sam sat on the cat.',\n",
       " 'The cat is white.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data.split('\\n')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,1)) # stop_words='english'\n",
    "tfidf = tfidf_vectorizer.fit_transform(dataset)  \n",
    "type(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4854606118156975\n",
      "  (0, 0)\t0.4854606118156975\n",
      "  (0, 7)\t0.3827427224171519\n",
      "  (0, 4)\t0.3827427224171519\n",
      "  (0, 12)\t0.4854606118156975\n",
      "  (1, 2)\t0.5552826649411127\n",
      "  (1, 5)\t0.5552826649411127\n",
      "  (1, 7)\t0.43779123108611473\n",
      "  (1, 4)\t0.43779123108611473\n",
      "  (2, 3)\t0.3827427224171519\n",
      "  (2, 11)\t0.3827427224171519\n",
      "  (2, 8)\t0.4854606118156975\n",
      "  (2, 10)\t0.4854606118156975\n",
      "  (2, 9)\t0.4854606118156975\n",
      "  (3, 13)\t0.5552826649411127\n",
      "  (3, 6)\t0.5552826649411127\n",
      "  (3, 3)\t0.43779123108611473\n",
      "  (3, 11)\t0.43779123108611473\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove_Vectors_Word_Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR_PATH ='./glove_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./glove_embeddings\\\\glove.50d.txt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(GLOVE_DIR_PATH, 'glove.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index1 = {}\n",
    "a = open(os.path.join(GLOVE_DIR_PATH, 'glove.50d.txt'),encoding=\"utf8\")\n",
    "\n",
    "for line in a:\n",
    "    values = line.split()\n",
    "    # print(values)\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index1[word] = coefs\n",
    "    \n",
    "a.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['the', ',', '.', 'of', 'to', 'and'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26818 ,  0.14346 , -0.27877 ,  0.016257,  0.11384 ,  0.69923 ,\n",
       "       -0.51332 , -0.47368 , -0.33075 , -0.13834 ,  0.2702  ,  0.30938 ,\n",
       "       -0.45012 , -0.4127  , -0.09932 ,  0.038085,  0.029749,  0.10076 ,\n",
       "       -0.25058 , -0.51818 ,  0.34558 ,  0.44922 ,  0.48791 , -0.080866,\n",
       "       -0.10121 , -1.3777  , -0.10866 , -0.23201 ,  0.012839, -0.46508 ,\n",
       "        3.8463  ,  0.31362 ,  0.13643 , -0.52244 ,  0.3302  ,  0.33707 ,\n",
       "       -0.35601 ,  0.32431 ,  0.12041 ,  0.3512  , -0.069043,  0.36885 ,\n",
       "        0.25168 , -0.24517 ,  0.25381 ,  0.1367  , -0.31178 , -0.6321  ,\n",
       "       -0.25028 , -0.38097 ], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index1['and']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = 'C:/Users/HP/Desktop/INTERNSHIP/KristiyanVachev/Question-Generation-master/data/embeddings'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.3069   ,  0.34337  , -0.6561   ,  0.50535  , -0.31266  ,\n",
       "       -0.016182 , -0.86545  ,  0.018588 ,  0.65697  ,  0.48712  ,\n",
       "        0.60409  , -0.14442  , -0.13581  , -0.37783  ,  0.13353  ,\n",
       "       -0.29716  ,  0.45541  ,  1.3479   ,  0.02077  ,  0.047824 ,\n",
       "       -0.059017 , -0.52721  , -0.12911  ,  0.36634  , -0.3437   ,\n",
       "        0.44807  ,  0.26396  , -0.03747  ,  0.091416 , -0.52161  ,\n",
       "        0.36524  ,  0.27812  , -0.23949  , -0.14362  ,  0.2543   ,\n",
       "        0.13902  , -0.26839  ,  0.27864  ,  0.23045  , -0.61099  ,\n",
       "        0.66523  ,  0.59624  , -0.63049  , -0.059215 , -0.93496  ,\n",
       "        0.41595  ,  0.27816  , -0.28668  , -0.4027   ,  0.43794  ,\n",
       "        0.0081997, -0.13542  ,  0.69342  ,  0.55247  , -0.031567 ,\n",
       "       -0.060406 ,  0.53747  ,  0.64762  ,  0.47787  ,  0.0064127,\n",
       "       -0.49562  , -0.38452  , -0.6629   ,  0.35706  ,  0.71461  ,\n",
       "        0.43798  ,  0.26165  , -0.26898  ,  0.5635   , -0.25969  ,\n",
       "       -0.61574  ,  0.2409   ,  0.29615  , -0.56231  ,  0.29118  ,\n",
       "       -0.16626  , -0.1762   , -0.8583   , -0.29637  ,  0.17477  ,\n",
       "       -0.27668  ,  0.052056 , -1.3648   , -0.4695   , -0.23453  ,\n",
       "        0.38541  ,  0.91612  , -0.45371  , -0.070356 ,  0.45356  ,\n",
       "       -0.56951  ,  1.0177   , -0.27158  , -0.081268 ,  0.19165  ,\n",
       "       -0.78914  ,  0.40251  , -0.14526  , -0.30298  ,  0.078954 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['compute']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 5 Words Closest To Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "u = embeddings_index['compute']\n",
    "similarity = []\n",
    "\n",
    "# cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "norm_u = norm(u)\n",
    "\n",
    "similarity = []\n",
    "\n",
    "for word in embeddings_index.keys():\n",
    "    v = embeddings_index[word]\n",
    "    cosine = dot(u, v)/(norm_u*norm(v))\n",
    "    similarity.append((word, cosine))\n",
    "\n",
    "print(len(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('compute', 0.99999994),\n",
       " ('calculate', 0.7222062),\n",
       " ('algorithm', 0.6441058),\n",
       " ('computed', 0.6136234),\n",
       " ('algorithms', 0.6134383),\n",
       " ('equivalently', 0.59991413),\n",
       " ('formula_1', 0.59704244),\n",
       " ('formula_2', 0.5948517)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(similarity, key=lambda x: x[1], reverse=True)[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
